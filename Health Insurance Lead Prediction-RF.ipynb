{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Health Insurance Lead Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x000001610C22CF08>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/xgboost/\n",
      "  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x000001610C22C4C8>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/xgboost/\n",
      "  WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x000001610C228508>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/xgboost/\n",
      "  WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x000001610C22E408>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/xgboost/\n",
      "  WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x000001610C22E988>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/xgboost/\n",
      "  ERROR: Could not find a version that satisfies the requirement xgboost (from versions: none)\n",
      "ERROR: No matching distribution found for xgboost\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-add1f35b42a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score,precision_score, recall_score,f1_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Train dataset and Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df =pd.read_csv(\"E:\\Analytics Vidhya\\Health Insurance Lead Prediction\\Data\\Train_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"E:\\Analytics Vidhya\\Health Insurance Lead Prediction\\Data\\Test_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Dataset_size :\" ,train_df.shape)\n",
    "print(\"Test Dataset_size  :\",test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify percitage of null values\n",
    "\n",
    "train_miss = train_df.isnull().sum() / len(train_df) * 100\n",
    "train_miss = train_miss[train_miss > 0]\n",
    "train_miss.sort_values(inplace=True)\n",
    "train_miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train_df.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_miss.plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify percitage of null values\n",
    "\n",
    "test_miss = test_df.isnull().sum() / len(test_df) * 100\n",
    "test_miss = test_miss[test_miss > 0]\n",
    "test_miss.sort_values(inplace=True)\n",
    "test_miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(test_df.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_miss.plot(kind = \"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = train_df.describe()\n",
    "summary = summary.transpose()\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_test = test_df.describe()\n",
    "summary_test = summary_test.transpose()\n",
    "summary_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Response'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(train_df['Response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelEncoder= LabelEncoder()\n",
    "train_df['City_Code'] = labelEncoder.fit_transform(train_df['City_Code'])\n",
    "test_df['City_Code'] = labelEncoder.fit_transform(test_df['City_Code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holding_Policy_Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Holding_Policy_Duration'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot('Holding_Policy_Duration', data = train_df, hue = 'Response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Holding_Policy_Duration'].replace(to_replace='\\+', value='', regex=True, inplace=True)\n",
    "train_df['Holding_Policy_Duration'] = pd.to_numeric(train_df['Holding_Policy_Duration'], errors='coerce')\n",
    "test_df['Holding_Policy_Duration'].replace(to_replace='\\+', value='', regex=True, inplace=True)\n",
    "test_df['Holding_Policy_Duration'] = pd.to_numeric(test_df['Holding_Policy_Duration'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accomodation_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Accomodation_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot('Accomodation_Type', data = train_df, hue = 'Response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Accomodation_Type'].replace(to_replace='Rented', value='0', regex=True, inplace=True)\n",
    "train_df['Accomodation_Type'].replace(to_replace='Owned', value='1', regex=True, inplace=True)\n",
    "test_df['Accomodation_Type'].replace(to_replace='Rented', value='0', regex=True, inplace=True)\n",
    "test_df['Accomodation_Type'].replace(to_replace='Owned', value='1', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reco_Insurance_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Reco_Insurance_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot('Reco_Insurance_Type', data = train_df, hue = 'Response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Reco_Insurance_Type'].replace(to_replace='Individual', value='0', regex=True, inplace=True)\n",
    "train_df['Reco_Insurance_Type'].replace(to_replace='Joint', value='1', regex=True, inplace=True)\n",
    "test_df['Reco_Insurance_Type'].replace(to_replace='Individual', value='0', regex=True, inplace=True)\n",
    "test_df['Reco_Insurance_Type'].replace(to_replace='Joint', value='1', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is_Spouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Is_Spouse'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot('Is_Spouse', data = train_df, hue = 'Response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Is_Spouse'].replace(to_replace='Yes', value='0', regex=True, inplace=True)\n",
    "train_df['Is_Spouse'].replace(to_replace='No', value='1', regex=True, inplace=True)\n",
    "test_df['Is_Spouse'].replace(to_replace='Yes', value='0', regex=True, inplace=True)\n",
    "test_df['Is_Spouse'].replace(to_replace='No', value='1', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Health Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Health Indicator'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Health Indicator'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Health Indicator'].replace(to_replace='X1', value='0', regex=True, inplace=True)\n",
    "train_df['Health Indicator'].replace(to_replace='X2', value='1', regex=True, inplace=True)\n",
    "train_df['Health Indicator'].replace(to_replace='X3', value='2', regex=True, inplace=True)\n",
    "train_df['Health Indicator'].replace(to_replace='X4', value='3', regex=True, inplace=True)\n",
    "train_df['Health Indicator'].replace(to_replace='X5', value='4', regex=True, inplace=True)\n",
    "train_df['Health Indicator'].replace(to_replace='X6', value='5', regex=True, inplace=True)\n",
    "train_df['Health Indicator'].replace(to_replace='X7', value='6', regex=True, inplace=True)\n",
    "train_df['Health Indicator'].replace(to_replace='X8', value='7', regex=True, inplace=True)\n",
    "train_df['Health Indicator'].replace(to_replace='X9', value='8', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Health Indicator'].replace(to_replace='X1', value='0', regex=True, inplace=True)\n",
    "test_df['Health Indicator'].replace(to_replace='X2', value='1', regex=True, inplace=True)\n",
    "test_df['Health Indicator'].replace(to_replace='X3', value='2', regex=True, inplace=True)\n",
    "test_df['Health Indicator'].replace(to_replace='X4', value='3', regex=True, inplace=True)\n",
    "test_df['Health Indicator'].replace(to_replace='X5', value='4', regex=True, inplace=True)\n",
    "test_df['Health Indicator'].replace(to_replace='X6', value='5', regex=True, inplace=True)\n",
    "test_df['Health Indicator'].replace(to_replace='X7', value='6', regex=True, inplace=True)\n",
    "test_df['Health Indicator'].replace(to_replace='X8', value='7', regex=True, inplace=True)\n",
    "test_df['Health Indicator'].replace(to_replace='X9', value='8', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holding_Policy_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Holding_Policy_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Holding_Policy_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot('Holding_Policy_Type', data = train_df, hue = 'Response')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Upper_Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_df['Upper_Age'], bins = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test_df['Upper_Age'], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Mean_Age\"] = (train_df.Upper_Age + train_df.Lower_Age)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower_Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_df['Lower_Age'], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test_df['Lower_Age'], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"Mean_Age\"] = (test_df.Upper_Age + test_df.Lower_Age)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Null Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Holding_Policy_Duration','Holding_Policy_Type','Health Indicator']\n",
    "for col in cols:\n",
    "    print('Imputation with Median: %s' % (col))\n",
    "    train_df[col].fillna(train_df[col].median(), inplace=True)\n",
    "    test_df[col].fillna(train_df[col].median(), inplace=True)\n",
    "    #X[col].fillna(0, inplace=True)\n",
    "    #x[col].fillna(0, inplace=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['Upper_Age','Lower_Age'], axis = 1)\n",
    "test_df = test_df.drop(['Upper_Age','Lower_Age'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting  X, Y, Train data and Test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop('Response', axis = 1)\n",
    "Y = train_df.Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomsample=  RandomOverSampler()\n",
    "x_new,y_new=randomsample.fit_resample(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_data= StandardScaler().fit_transform(x_new.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(sc_data), np.array(y_new), test_size=0.30)\n",
    "#eval_set=[(X_test, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG Boost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XG_model = XGBClassifier(objective=\"binary:logistic\", learning_rate=0.05, seed=9616, \n",
    "                                       max_depth=20, gamma=10, n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_model = XG_model.fit(X_train, y_train, early_stopping_rounds=50, eval_metric=\"auc\", eval_set=eval_set, verbose=True)\n",
    "xg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train accuracy',xg_model.score(X_train, y_train))\n",
    "print('Test accuracy',xg_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_pred = xg_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_cm = confusion_matrix(y_test, xg_pred)\n",
    "print(xg_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_classification_report = classification_report(y_test, xg_pred)\n",
    "print(xg_classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  XG Boost Train accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_pred = pd.DataFrame( { 'actual':  y_train,   'predicted': XG_model.predict( X_train ) } )\n",
    "xg_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_cm1 = metrics.confusion_matrix( xg_pred.actual,  xg_pred.predicted)\n",
    "xg_cm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = classification_report(xg_pred.actual,  xg_pred.predicted)\n",
    "print(xg_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy_score is  :\", accuracy_score(xg_pred.actual,  xg_pred.predicted))\n",
    "print(\"precision score is :\",precision_score(xg_pred.actual,  xg_pred.predicted))\n",
    "print(\"recall is          :\",recall_score(xg_pred.actual,  xg_pred.predicted))\n",
    "print(\"F1_score is        :\",f1_score(xg_pred.actual,   xg_pred.predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"ROC_AUC Score :\", roc_auc_score(xg_pred.actual,  xg_pred.predicted))\n",
    "\n",
    "fpr,tpr,thresholds=roc_curve(xg_pred.actual,  xg_pred.predicted)\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(fpr,tpr,color='blue',label='ROC')\n",
    "plt.plot([0,1],[0,1],color='green',linestyle='--')\n",
    "plt.title('ROC Curve of  XG Boost Model')\n",
    "plt.xlabel('FalsePositiveRate')\n",
    "plt.ylabel('TruePositiveRate')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_test_pred = pd.DataFrame( { 'actual':  y_test, 'predicted': XG_model.predict( X_test ) } )\n",
    "xg_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_cm1 = metrics.confusion_matrix( xg_test_pred.actual,   xg_test_pred.predicted)\n",
    "print(xg_cm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy_score is  :\", accuracy_score(xg_test_pred.actual,  xg_test_pred.predicted))\n",
    "print(\"precision score is :\",precision_score(xg_test_pred.actual,  xg_test_pred.predicted))\n",
    "print(\"recall is          :\",recall_score(xg_test_pred.actual,    xg_test_pred.predicted))\n",
    "print(\"F1_score is        :\",f1_score(xg_test_pred.actual,  xg_test_pred.predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"ROC_AUC Score :\", roc_auc_score(xg_test_pred.actual,  xg_test_pred.predicted))\n",
    "\n",
    "fpr,tpr,thresholds=roc_curve(xg_test_pred.actual,  xg_test_pred.predicted)\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(fpr,tpr,color='blue',label='ROC')\n",
    "plt.plot([0,1],[0,1],color='green',linestyle='--')\n",
    "plt.title('ROC Curve of  XG Boost Model')\n",
    "plt.xlabel('FalsePositiveRate')\n",
    "plt.ylabel('TruePositiveRate')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
    "                       n_jobs=None, oob_score=False, random_state=None,\n",
    "                       verbose=0, warm_start=False)\n",
    "rf_model = rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train accuracy :',rf_model.score(X_train, y_train))\n",
    "print('Test accuracy  :',rf_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model Accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran_forest_pred = rf_model.predict(X_test)\n",
    "print(ran_forest_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran_forest_cm = confusion_matrix(y_test, ran_forest_pred)\n",
    "print(ran_forest_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_classification_report = classification_report(y_test, ran_forest_pred)\n",
    "print(random_forest_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy_score is  :\", accuracy_score(y_test, ran_forest_pred))\n",
    "print(\"precision score is :\",precision_score(y_test, ran_forest_pred))\n",
    "print(\"recall is          :\",recall_score(y_test, ran_forest_pred))\n",
    "print(\"F1_score is        :\",f1_score(y_test, ran_forest_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"ROC_AUC Score :\",roc_auc_score(y_test, ran_forest_pred))\n",
    "\n",
    "fpr,tpr,thresholds=roc_curve(y_test, ran_forest_pred)\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(fpr,tpr,color='blue',label='ROC')\n",
    "plt.plot([0,1],[0,1],color='green',linestyle='--')\n",
    "plt.title('ROC Curve of  Random Forest Model')\n",
    "plt.xlabel('FalsePositiveRate')\n",
    "plt.ylabel('TruePositiveRate')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate = []\n",
    "\n",
    "for i in range(1,20):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred_i = knn.predict(X_test)\n",
    "    error_rate.append(np.mean(y_pred_i != y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(range(1,20),error_rate,color='blue',ls='--',marker='.',markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors = 5, p=2)\n",
    "knn_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train accuracy :',knn_model.score(X_train, y_train))\n",
    "print('Test accuracy  :',knn_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Model Accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion_Matrix :\")\n",
    "print(confusion_matrix(y_test,knn_pred))\n",
    "print(\"Classifisction_report :\")\n",
    "print('\\n')\n",
    "print(classification_report(y_test,knn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
